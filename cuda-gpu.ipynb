{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:00:48.811597Z","iopub.status.idle":"2025-02-17T18:00:48.811990Z","shell.execute_reply":"2025-02-17T18:00:48.811802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n!nvcc --version\n!pip install nvcc4jupyter\n%load_ext nvcc4jupyter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:52:39.210967Z","iopub.execute_input":"2025-02-19T08:52:39.211184Z","iopub.status.idle":"2025-02-19T08:54:31.457995Z","shell.execute_reply.started":"2025-02-19T08:52:39.211162Z","shell.execute_reply":"2025-02-19T08:54:31.456927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:54:31.462201Z","iopub.execute_input":"2025-02-19T08:54:31.462438Z","iopub.status.idle":"2025-02-19T08:54:31.581721Z","shell.execute_reply.started":"2025-02-19T08:54:31.462416Z","shell.execute_reply":"2025-02-19T08:54:31.580823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!g++ -v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:49:41.391527Z","iopub.execute_input":"2025-02-18T07:49:41.391789Z","iopub.status.idle":"2025-02-18T07:49:41.516411Z","shell.execute_reply.started":"2025-02-18T07:49:41.391756Z","shell.execute_reply":"2025-02-18T07:49:41.515661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile cpphw.hpp\n\n#include <iostream>\n\nvoid printHelloWorld()\n\n{\n\nstd::cout << \"HelloWorld\\nHelloWorld\";\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:02.460501Z","iopub.execute_input":"2025-02-16T08:52:02.460811Z","iopub.status.idle":"2025-02-16T08:52:02.466794Z","shell.execute_reply.started":"2025-02-16T08:52:02.460774Z","shell.execute_reply":"2025-02-16T08:52:02.466012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile cpphw.cpp\n\n#include \"cpphw.hpp\"\n\nint main()\n\n{\n\nprintHelloWorld();\n\nreturn 0;\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:02.467816Z","iopub.execute_input":"2025-02-16T08:52:02.468085Z","iopub.status.idle":"2025-02-16T08:52:02.480337Z","shell.execute_reply.started":"2025-02-16T08:52:02.468048Z","shell.execute_reply":"2025-02-16T08:52:02.479536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:08:47.806427Z","iopub.execute_input":"2025-02-11T05:08:47.806741Z","iopub.status.idle":"2025-02-11T05:08:47.925064Z","shell.execute_reply.started":"2025-02-11T05:08:47.806718Z","shell.execute_reply":"2025-02-11T05:08:47.924175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\ng++ cpphw.cpp -std=c++11 -o hw.out\n./hw.out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:08:56.090246Z","iopub.execute_input":"2025-02-11T05:08:56.090696Z","iopub.status.idle":"2025-02-11T05:08:56.389494Z","shell.execute_reply.started":"2025-02-11T05:08:56.090665Z","shell.execute_reply":"2025-02-11T05:08:56.388755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 1 (16 Feb, 2025)","metadata":{}},{"cell_type":"code","source":"%%writefile vect_add_cpu.cpp\n// Sequential Vector Addition in CPP\n\n#include <iostream>\n#include <vector>\n#include <ctime>\nusing namespace std;\n\nint main(){\n    clock_t start = clock();\n    int n = 200000000;\n    vector<int> A(n), B(n), C(n);\n    for (int i=0; i<n; i++){\n        A[i] = i+1;\n        B[i] = (i+1)*2;\n    }\n\n    for (int i=0;i<n; i++){\n        C[i] = A[i] + B[i];\n\n    }\n\n    clock_t end = clock();\n\n    double duration = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"Time taken: \" << duration * 1000 << \" milliseconds\" << std::endl; // Convert to milliseconds\n\n    for (int i=0;i <20; i++){\n        cout << C[i] << \" \";\n    }\n    return 0;\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:48:57.183546Z","iopub.execute_input":"2025-02-16T09:48:57.183884Z","iopub.status.idle":"2025-02-16T09:48:57.189127Z","shell.execute_reply.started":"2025-02-16T09:48:57.183856Z","shell.execute_reply":"2025-02-16T09:48:57.188139Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\ng++ vect_add_cpu.cpp -std=c++11 -o cpu.out\n./cpu.out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:48:58.107757Z","iopub.execute_input":"2025-02-16T09:48:58.108042Z","iopub.status.idle":"2025-02-16T09:49:04.641248Z","shell.execute_reply.started":"2025-02-16T09:48:58.108021Z","shell.execute_reply":"2025-02-16T09:49:04.640176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile vect_add_gpu.cu\n// vector addition on gpu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void vector_addition(int *A, int *B, int *C, int n){\n    // std::cout << \"blockIdx.x = \" << blockIdx.x << endl ;\n    // std::cout << \"blockDim.x = \" << blockDim.x << endl ;\n    // std::cout << \"threadIdx.x = \" << threadIdx.x << endl ;\n\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i<n){\n        C[i] = A[i] + B[i];\n    }\n    \n};\n\nint main(){\n    int n = 200000000;\n    vector<int> A(n), B(n), C(n);\n    for (int i=0; i<n; i++){\n        A[i] = i+1;\n        B[i] = (i+1)*2;\n    }\n\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    cudaMalloc((void **)&d_A, n*sizeof(int));\n    cudaMalloc((void **)&d_B, n*sizeof(int));\n    cudaMalloc((void **)&d_C, n*sizeof(int));\n\n    cudaMemcpy(d_A, A.data(), n*sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B.data(), n*sizeof(int), cudaMemcpyHostToDevice);\n\n    int blockSize = 2;\n    int gridSize = n+blockSize - 1;\n    \n    vector_addition <<<gridSize, blockSize>>>(d_A, d_B, d_C, n);\n    cudaMemcpy(C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost);\n\n    clock_t end = clock();\n\n    double duration = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"Time taken: \" << duration * 1000 << \" milliseconds\" << std::endl; // Convert to milliseconds\n\n    for (int i=0;i <20; i++){\n        cout << C[i] << \" \";\n    }\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n    \n    return 0;  \n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:07:07.363220Z","iopub.execute_input":"2025-02-16T11:07:07.363529Z","iopub.status.idle":"2025-02-16T11:07:07.369766Z","shell.execute_reply.started":"2025-02-16T11:07:07.363507Z","shell.execute_reply":"2025-02-16T11:07:07.368812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\nnvcc vect_add_gpu.cu -o kernel\n./kernel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:07:11.333063Z","iopub.execute_input":"2025-02-16T11:07:11.333322Z","iopub.status.idle":"2025-02-16T11:07:19.223138Z","shell.execute_reply.started":"2025-02-16T11:07:11.333301Z","shell.execute_reply":"2025-02-16T11:07:19.222288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notes:\n* Gird Dimension: gridDim.x, gridDim.y, gridDim.z is the number of blocks in each direction of Grid.\n* Block Dimension: blockDim.x, blockDim.y, blockDim.z, is the number of threads in each direction of block.\n* Block Index: blockIdx.x[y,z], index of the block within the grid\n* Thread Index: threadIdx.x[y,z], index of thread within a block\n  \nthreadId =\n\n            blockDim.x\\*blockIdx.x + blockDim.x\\*threadIdx.y + threadIdx.x (simple 2D within a block) +\n              \n            gridDim.x\\*blockDim.y\\*blockIdx.y (blocks of threads in previous row) \n\n\n**Steps:**\n1. Define the function to run on GPU.\n2. Declare variables on CPU and allocate necessary memory on GPU\n3. Copy the data from CPU to GPU on the allocated locations.\n4. Call the kernel\n5. Save the copy of the task result on the CPU.\n6. free the memory on GPU\n","metadata":{}},{"cell_type":"markdown","source":"# Day 2 (17 Feb, 2025)","metadata":{"execution":{"iopub.status.busy":"2025-02-17T17:26:03.433641Z","iopub.execute_input":"2025-02-17T17:26:03.434097Z","iopub.status.idle":"2025-02-17T17:26:03.439567Z","shell.execute_reply.started":"2025-02-17T17:26:03.434055Z","shell.execute_reply":"2025-02-17T17:26:03.437465Z"}}},{"cell_type":"code","source":"%%writefile vect_2D_add_gpu.cu\n// vector addition on gpu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void vector_addition(int *A, int *B, int *C, int rows, int cols){\n    // std::cout << \"blockIdx.x = \" << blockIdx.x << endl ;\n    // std::cout << \"blockDim.x = \" << blockDim.x << endl ;\n    // std::cout << \"threadIdx.x = \" << threadIdx.x << endl ;\n\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < rows && col < cols){\n        int index = row*cols + col;\n       // std::cout << index << endl;\n        C[index] = A[index] + B[index];\n    }\n    \n};\n\nvoid print_vector(vector<vector <int>> matrix){\n  // vector<vector<int>> matrix(3);\n    for (vector vec: matrix){\n      for (int val : vec){\n        cout << val << \" \";\n      }\n      cout << endl;\n    }\n}\n\n// Helper function to check for CUDA errors\nvoid checkCudaError(cudaError_t error) {\n    if (error != cudaSuccess) {\n        std::cerr << \"CUDA Error: \" << cudaGetErrorString(error) << std::endl;\n        exit(EXIT_FAILURE);\n    }\n}\n\nint main(){\n    \n    int rows = 30000;\n    int cols = 30000;\n    int n = rows*cols;\n    vector<int> h_A(n), h_B(n), h_C(n);\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_A[i * cols + j] = 2; // Example initialization for A\n            h_B[i * cols + j] = -1;\n                }\n    }\n\n    clock_t start_cpu = clock();\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_C[i * cols + j] = h_A[i * cols + j] + h_B[i * cols + j];\n                }\n    }\n    clock_t end_cpu = clock();\n    double duration_cpu = static_cast<double>(end_cpu - start_cpu) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"CPU Time taken: \" << duration_cpu * 1000 << \" milliseconds\" << std::endl; \n\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    checkCudaError(cudaMalloc((void **)&d_A, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_B, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_C, n*sizeof(int)));\n\n    checkCudaError(cudaMemcpy(d_A, h_A.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n    checkCudaError(cudaMemcpy(d_B, h_B.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n\n    // dim3 blockSize(rows, cols); // Block dimension (threads per block in x and y) - adjust based on matrix size and GPU capabilities\n    // dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n\n    dim3 blockSize(16, 16); // Example block size\n    blockSize.x = min(blockSize.x, cols); // Clamp block size to cols\n    blockSize.y = min(blockSize.y, rows); // Clamp block size to rows\n    dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n    \n    vector_addition <<<gridSize, blockSize>>>(d_A, d_B, d_C, rows, cols);\n    checkCudaError(cudaGetLastError());\n    clock_t d2h_start = clock();\n    checkCudaError(cudaMemcpy(h_C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost));\n    clock_t d2h_end = clock();\n    double d2h_duration = static_cast<double>(d2h_end - d2h_start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"MemCpy D2H Time taken: \" << d2h_duration * 1000 << \" milliseconds\" << std::endl; \n\n    clock_t end = clock();\n\n    double duration_cuda = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"CUDA Time taken: \" << duration_cuda * 1000 << \" milliseconds\" << std::endl; \n    // Convert to milliseconds\n\n    cout << \"CUDA Matrix Addition (2D):\" << endl;\n    \n    for (int i = 0; i < 10; ++i) {\n        for (int j = 0; j < 10; ++j) {\n            cout << h_C[i * cols + j] << \" \";\n        }\n        cout << endl;\n    }\n    checkCudaError(cudaFree(d_A));\n    checkCudaError(cudaFree(d_B));\n    checkCudaError(cudaFree(d_C));\n    checkCudaError(cudaDeviceSynchronize());\n    \n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:37.930699Z","iopub.execute_input":"2025-02-18T08:38:37.931034Z","iopub.status.idle":"2025-02-18T08:38:37.936322Z","shell.execute_reply.started":"2025-02-18T08:38:37.931004Z","shell.execute_reply":"2025-02-18T08:38:37.935625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\nnvcc vect_2D_add_gpu.cu -o vect_2d\n./vect_2d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:41.025379Z","iopub.execute_input":"2025-02-18T08:38:41.025651Z","iopub.status.idle":"2025-02-18T08:39:14.104315Z","shell.execute_reply.started":"2025-02-18T08:38:41.025629Z","shell.execute_reply":"2025-02-18T08:39:14.103417Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 3 (18 Feb, 2025)","metadata":{}},{"cell_type":"markdown","source":"Chapter 3 Notes:\n* dim3 dimGrid(32,1,1), dim dimBlock(128,1,1) or dim3 dog(32,1,1), dim3 cat(128,1,1)\n* vec_add <<<dimGrid or dog, dimBlock or cat>>> // we can use both with dim3 type, it is vector of int type with 3 elements, x, y, z.\n* \"For convenience, CUDA provides a special shortcut for calling a kernel with one-dimensional (1D) grids and blocks. Instead of using dim3 variables, one can use arithmetic expressions to specify the configuration of 1D grids and blocks. In this case, the CUDA compiler simply takes the arithmetic expression as the x dimensions and assumes that the y and z dimensions are 1.\" Page 49\n* \"the gridDim and blockDim are built-in variables in a kernel and always reflect the dimensions of the grid and the blocks, respectively.\"\n* the allowed values of gridDim.x range from 1 to 231 2 1,1 and those of gridDim.y and gridDim.z range from 1 to 216 2 1 (65,535).\n* The total size of a block in current CUDA systems is limited to 1024 threads. These threads can be distributed across the three dimensions in any way as long as the total number of threads does not exceed 1024.\n* We need to know the no of cols at the compile time to accept dynamically allocated  arrays, but we do not have this info, as a result we flattne the dynamically allocated 2D arrays into an equivalent 1D arrays.\n* The linearized access to a 3D array P will be in the form of P[plane * m * n +row * m + col].","metadata":{}},{"cell_type":"code","source":"%%writefile matrix_mul.cu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void mat_mul(int *P_A, int *P_B,int *P_C, int Width){\n    int row = blockDim.y*blockIdx.y + threadIdx.y;\n    int col = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (row < Width && col < Width){\n        for (int k=0; k<Width; k++){\n            P_C[row*Width+col] += P_A[row*Width+k] * P_B[k*Width+col];\n        }\n    }\n}\n\nvoid checkCudaError(cudaError_t error) {\n    if (error != cudaSuccess) {\n        std::cerr << \"CUDA Error: \" << cudaGetErrorString(error) << std::endl;\n        exit(EXIT_FAILURE);\n    }\n}\nint main(){\n    int rows = 30000;\n    int cols = 30000;\n    int n = rows*cols;\n    vector<int> h_A(n), h_B(n), h_C(n);\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_A[i * cols + j] = 1; // Example initialization for A\n            h_B[i * cols + j] = 2;\n                }\n    }\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    checkCudaError(cudaMalloc((void **)&d_A, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_B, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_C, n*sizeof(int)));\n\n    checkCudaError(cudaMemcpy(d_A, h_A.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n    checkCudaError(cudaMemcpy(d_B, h_B.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n\n    int width = rows;\n    dim3 blockSize(32, 32); // Example block size\n    blockSize.x = min(blockSize.x, cols); // Clamp block size to cols\n    blockSize.y = min(blockSize.y, rows); // Clamp block size to rows\n    dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n\n    clock_t gpu_start = clock();\n    mat_mul <<<gridSize, blockSize>>>(d_A, d_B, d_C, width);\n    clock_t gpu_end = clock();\n    double gpu_duration = static_cast<double>(gpu_end - gpu_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"GPU Time taken: \" << gpu_duration * 1000 << \" milliseconds\" << std::endl; \n    \n    clock_t d2h_start = clock();\n    checkCudaError(cudaMemcpy(h_C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost));\n    clock_t d2h_end = clock();\n    double d2h_duration = static_cast<double>(d2h_end - d2h_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"MemCpy D2H Time taken: \" << d2h_duration * 1000 << \" milliseconds\" << std::endl; \n\n    checkCudaError(cudaGetLastError());\n    cout << \"CUDA Matrix Mul (2D):\" << endl;\n    \n    for (int i = 0; i < 10; ++i) {\n        for (int j = 0; j < 10; ++j) {\n            cout << h_C[i * cols + j] << \" \";\n        }\n        cout << endl;\n    }\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:48:25.889545Z","iopub.execute_input":"2025-02-19T06:48:25.889870Z","iopub.status.idle":"2025-02-19T06:48:25.895572Z","shell.execute_reply.started":"2025-02-19T06:48:25.889844Z","shell.execute_reply":"2025-02-19T06:48:25.894825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\nnvcc matrix_mul.cu -o mul\n./mul","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T06:48:29.250064Z","iopub.execute_input":"2025-02-19T06:48:29.250472Z","iopub.status.idle":"2025-02-19T06:53:11.352774Z","shell.execute_reply.started":"2025-02-19T06:48:29.250438Z","shell.execute_reply":"2025-02-19T06:53:11.352032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T07:16:35.134160Z","iopub.execute_input":"2025-02-19T07:16:35.134447Z","iopub.status.idle":"2025-02-19T07:16:35.253227Z","shell.execute_reply.started":"2025-02-19T07:16:35.134425Z","shell.execute_reply":"2025-02-19T07:16:35.252413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T08:33:31.916622Z","iopub.execute_input":"2025-02-19T08:33:31.916950Z","iopub.status.idle":"2025-02-19T08:33:32.140735Z","shell.execute_reply.started":"2025-02-19T08:33:31.916919Z","shell.execute_reply":"2025-02-19T08:33:32.139469Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 4 (19 Feb, 2025)","metadata":{}},{"cell_type":"markdown","source":"### Notes\n#### Shared Memory\n* On-chip memory that is physically located close to the GPU cores, unlike global memory (DRAM) which is off-chip.\n* Low Latency, High bandwith, Shared within a block, Limited Size.","metadata":{}},{"cell_type":"code","source":"!python --version\n!nvcc --version\n!pip install nvcc4jupyter\n%load_ext nvcc4jupyter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:31:07.313608Z","iopub.execute_input":"2025-02-20T08:31:07.313907Z","iopub.status.idle":"2025-02-20T08:32:47.321516Z","shell.execute_reply.started":"2025-02-20T08:31:07.313885Z","shell.execute_reply":"2025-02-20T08:32:47.320495Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\nCollecting nvcc4jupyter\n  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\nDownloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\nDetected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmp_jsmuhxr\".\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile red.cu\n#include <iostream>\n#include <vector>\n#include <cuda_runtime.h>\n#include <ctime>\n\nusing namespace std;\nvoid checkCudaError(cudaError_t error){\n    if (error != cudaSuccess){\n        cerr << \"Cuda Error: \" << cudaGetErrorString(error) << endl;\n        exit(EXIT_FAILURE);\n    }\n}\n\n__global__ void reduction_tree(float *input, float*output, int block_size){\n\n    __shared__ float shared_data[256];\n\n    int thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    shared_data[thread_id] = input[block_id*block_size + thread_id];\n    __syncthreads();\n\n    for (int stride=block_size / 2; stride > 0; stride/=2){\n        if (thread_id < stride){\n            shared_data[thread_id] += shared_data[thread_id + stride];\n        }\n        __syncthreads();\n    }\n    if (thread_id == 0){\n        output[block_id] = shared_data[0];\n    }\n}\n\nint main(){\n    int array_size = 1024*1024*4*4;\n    int block_size = 32;\n\n    vector<float> h_input(array_size);\n    vector<float> h_output((array_size + block_size - 1) / block_size, 0.0f);\n\n    for (int i=0; i < array_size; i++){\n        // h_input[i] = (float) (i%10+1);\n        h_input[i] = 1;\n    }\n    \n    float *d_input, *d_output;\n    checkCudaError(cudaMalloc((void**)&d_input, array_size * sizeof(float)));\n    checkCudaError(cudaMalloc((void**)&d_output, h_output.size() * sizeof(float)));\n\n    checkCudaError(cudaMemcpy(d_input, h_input.data(), array_size * sizeof(float), cudaMemcpyHostToDevice));\n\n    dim3 blockDim(block_size);\n    dim3 gridDim((array_size+block_size-1)/block_size);\n\n    clock_t cpu_start = clock();\n    int array_sum = 0;\n    for (int i=0; i < array_size; i++){\n        array_sum += h_input[i];\n    }\n    clock_t cpu_end = clock();\n    double cpu_duration = static_cast<double>(cpu_end - cpu_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"CPU Time taken: \" << cpu_duration * 1000 << \" milliseconds\" << std::endl; \n\n    clock_t gpu_start = clock();\n    reduction_tree <<<gridDim, blockDim>>>(d_input, d_output, block_size);\n    clock_t gpu_end = clock();\n    double gpu_duration = static_cast<double>(gpu_end - gpu_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"GPU Time taken: \" << gpu_duration * 1000 << \" milliseconds\" << std::endl; \n    checkCudaError(cudaMemcpy(h_output.data(), d_output, h_output.size() * sizeof(float), cudaMemcpyDeviceToHost));\n\n    int gpu_sum = 0;\n    for (int i = 0; i < h_output.size(); i++){\n        gpu_sum += h_output[i];\n    }\n    cout << \"shared memory array size: \" << 256 << endl;\n    cout << \"gpu sum \" << gpu_sum<< endl;\n    cout << \"cpu sum \" << array_sum << endl;\n    checkCudaError(cudaFree(d_input));\n    checkCudaError(cudaFree(d_output));\n    \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:44:48.537321Z","iopub.execute_input":"2025-02-20T08:44:48.537651Z","iopub.status.idle":"2025-02-20T08:44:48.543238Z","shell.execute_reply.started":"2025-02-20T08:44:48.537625Z","shell.execute_reply":"2025-02-20T08:44:48.542495Z"}},"outputs":[{"name":"stdout","text":"Overwriting red.cu\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"%%script bash\nnvcc red.cu -o red\n./red","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T08:44:51.111116Z","iopub.execute_input":"2025-02-20T08:44:51.111479Z","iopub.status.idle":"2025-02-20T08:44:53.673327Z","shell.execute_reply.started":"2025-02-20T08:44:51.111451Z","shell.execute_reply":"2025-02-20T08:44:53.672447Z"}},"outputs":[{"name":"stdout","text":"CPU Time taken: 108.922 milliseconds\nGPU Time taken: 0.193 milliseconds\nshared memory array size: 256\ngpu sum 16777216\ncpu sum 16777216\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"{32: 0.225 milliseconds,\n 64: 0.219,\n 128: 0.182,\n 256: 0.212,\n 512:\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 5 (Feb 20, 2025)\n* with blockSize > 256 when shared memory is 256, we are likely writing out of bounds of the sharedData array, leading to undefined behavior and potential errors.\n* dynamic shared memory, you declare it as extern __shared__ float sharedData[]; in the kernel and then specify the size in bytes when you launch the kernel using the sharedMemConfig parameter of the kernel launch <<<>>>.","metadata":{}},{"cell_type":"code","source":"    for (int i=0; i < neurons; i++ ){\n        for (int j=0; j < sample; j++){\n            cout << i << j << \" \" ;\n            cout <<  neurons*i + j << endl;\n            \n            h_input[neurons*i + j] = neurons*i + j;\n        }\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile navie_layer_norm.cpp\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <stdio.h>\n#include <cmath>\n\nusing namespace std;\nvoid print_matrix(vector<float> matrix, int neurons, int sample ){\n    for (int i=0; i < neurons; i++ )\n        {\n        for (int j=0; j < sample; j++)\n        {\n            cout <<  matrix[sample*i + j] << \" \";\n        }\n        cout << endl ;\n        }\n    }\nint main()\n    {\n    //ex1 - a11, a21, a31; ex2 - a12, a22, a32, mean1 = (a11 + a12)/2\n    \n    int sample = 100000;  // cols\n    int neurons = 32; // rows\n\n    int array_size = sample * neurons;\n    vector<float> h_input(array_size);\n    \n    for (int i=0; i < neurons; i++ ){\n        for (int j=0; j < sample; j++){\n            h_input[sample*i + j] = sample*i + j;\n        }\n    }\n\n    // printf(\"input matrix \\n\");\n    // print_matrix(h_input, neurons, sample);\n\n    vector<float> h_mean(sample);\n    vector<float> h_var(sample, 0);\n    vector<float> h_std(sample, 0);\n    \n    for (int i=0; i < neurons; i++ ){\n        float col_sum = 0;\n        for (int j=0; j < sample; j++){\n            h_mean[j] += h_input[sample*i + j]; \n            }\n        }\n    for (int j=0; j < sample; j++){\n         h_mean[j] = h_mean[j]/neurons;\n        }\n//    printf(\"col mean: \\n\");\n//    for (float val: h_mean){\n//        cout << (val) << \" \";\n//    }\n//    printf(\"\\n\");\n\n    for (int i=0; i < neurons; i++ ){\n        for (int j=0; j < sample; j++){ \n            h_var[j] += ( h_input[sample*i + j] - h_mean[j]) * ( h_input[sample*i + j] - h_mean[j]); \n            }\n        }\n\n    for (int j=0; j < sample; j++){\n         h_std[j] = sqrt(h_var[j]/neurons);\n        }\n\n//   printf(\"col diff sqr sum: \\n\");\n//   for (float val: h_var){\n//       cout << (val) << \" \";\n//   }\n//   printf(\"\\n\");\n//\n//   printf(\"col std: \\n\");\n//   for (float val: h_std){\n//       cout << (val) << \" \";\n//   }\n//   printf(\"\\n\\nprinting output matrix \\n\\n\");\n//\n    clock_t cpu_start = clock();\n    vector<float> h_output(array_size,0);\n    for (int i=0; i < neurons; i++ ){\n        for (int j=0; j < sample; j++){\n            h_output[sample*i + j] = (h_input[sample*i + j] - h_mean[j])/(h_std[j] + 1e-7);\n            float val =  h_output[sample*i + j];\n            // cout << (val) << \" \";\n        }\n        // cout << endl;\n    }\n   \n    \n    for (int i=0; i < neurons; i++ ){\n        for (int j=0; j < sample; j++){\n            h_output[sample*i + j] = (h_input[sample*i + j] - h_mean[j])/(h_std[j] + 1e-7);\n            float val =  h_output[sample*i + j];\n            // cout << (val) << \" \";\n        }\n        // cout << endl;\n    }\n    clock_t cpu_end = clock();\n    double cpu_duration = static_cast<double>(cpu_end - cpu_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"CPU Time taken: \" << cpu_duration * 1000 << \" milliseconds\" << std::endl; \n\n    return 0;\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:04:32.737160Z","iopub.execute_input":"2025-02-21T10:04:32.737451Z","iopub.status.idle":"2025-02-21T10:04:32.742347Z","shell.execute_reply.started":"2025-02-21T10:04:32.737429Z","shell.execute_reply":"2025-02-21T10:04:32.741590Z"}},"outputs":[{"name":"stdout","text":"Overwriting navie_layer_norm.cpp\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"%%script bash\ng++ navie_layer_norm.cpp -o layer\n./layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:04:36.938366Z","iopub.execute_input":"2025-02-21T10:04:36.938643Z","iopub.status.idle":"2025-02-21T10:04:37.630783Z","shell.execute_reply.started":"2025-02-21T10:04:36.938622Z","shell.execute_reply":"2025-02-21T10:04:37.630108Z"}},"outputs":[{"name":"stdout","text":"CPU Time taken: 107.221 milliseconds\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!python --version\n!nvcc --version\n!pip install nvcc4jupyter\n%load_ext nvcc4jupyter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T08:37:43.735127Z","iopub.execute_input":"2025-02-21T08:37:43.735491Z","iopub.status.idle":"2025-02-21T08:39:39.423926Z","shell.execute_reply.started":"2025-02-21T08:37:43.735465Z","shell.execute_reply":"2025-02-21T08:39:39.423045Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\nCollecting nvcc4jupyter\n  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\nDownloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\nDetected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmpn057f3x1\".\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 6 (Feb 21, 2025)","metadata":{}},{"cell_type":"code","source":"%%writefile layerNorm.cu\n#include <vector>\n#include <iostream>\n#include <ctime>\n#include <cuda_runtime.h>\n#include <cmath>\n\nusing namespace std;\n\n__global__ void layer_norm(float *data, float *mean, float *var, float *output,  int neurons, int sample){\n    int idx = blockIdx.x *  blockDim.x + threadIdx.x;\n   // std::cout << idx;\n    if (idx  <  sample*neurons){\n        mean[idx] = 0.0f;\n        var[idx] = 0.0f;\n        for (int i=0; i < neurons; i++ ){\n\n            int row_idx = idx*neurons + i;\n            mean[idx] += data[row_idx];\n            \n        }\n        mean[idx] /= neurons;\n        for (int i=0; i < neurons; i++ ){\n\n            int row_idx = idx*neurons + i;\n            var[idx] += ((data[row_idx]-mean[idx]) * (data[row_idx]-mean[idx]));\n            \n        }\n        var[idx] /= neurons;\n        for (int i=0; i < neurons; i++ ){\n\n            int row_idx = idx*neurons + i;\n            output[row_idx] = (data[row_idx] - mean[idx])/(sqrt(var[idx])+1e-7);\n            \n        }\n        }\n}\n\nint main(){\n    int sample = 500000;\n    int neurons = 30;\n    vector<float> h_data(sample*neurons);\n    vector<float> h_out(sample*neurons);\n    vector<float> h_mean(sample);\n    vector<float> h_var(sample); \n\n    for (int i =0; i < sample; i++){\n        for (int j =0; j < neurons; j++){\n        h_data[i*neurons + j] = i*neurons + j;\n        }\n    }\n\n    float *d_data, *d_out, *d_mean, *d_var;\n    cudaMalloc((void **)&d_data, sample*neurons*sizeof(float));\n    cudaMalloc((void **)&d_out, sample*neurons*sizeof(float));\n    cudaMalloc((void **)&d_mean, sample*sizeof(float));\n    cudaMalloc((void **)&d_var, sample*sizeof(float));\n\n    cudaMemcpy(d_data, h_data.data(), sample*neurons*sizeof(float), cudaMemcpyHostToDevice );\n    cudaMemcpy(d_out, h_out.data(), sample*neurons*sizeof(float), cudaMemcpyHostToDevice );\n    cudaMemcpy(d_mean, h_mean.data(), sample*sizeof(float), cudaMemcpyHostToDevice );\n    cudaMemcpy(d_var, h_var.data(), sample*sizeof(float), cudaMemcpyHostToDevice );\n\n    dim3 blockSize(32);\n    dim3 gridSize((sample*neurons + blockSize.x - 1) / blockSize.x);  // Proper grid size calculation\n\n    clock_t cpu_start = clock();\n    layer_norm<<<gridSize, blockSize>>>(d_data, d_mean, d_var, d_out, neurons, sample);\n    clock_t cpu_end = clock();\n    double cpu_duration = static_cast<double>(cpu_end - cpu_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"gpu Time taken: \" << cpu_duration  << \" seconds\" << std::endl; \n    \n    cudaMemcpy(h_mean.data(), d_mean, sample*sizeof(float), cudaMemcpyDeviceToHost );\n    cudaMemcpy(h_var.data(), d_var, sample*sizeof(float), cudaMemcpyDeviceToHost );\n    cudaMemcpy(h_out.data(), d_out, sample*neurons*sizeof(float), cudaMemcpyDeviceToHost );\n\n\n    cudaFree(d_out);\n    cudaFree(d_mean);\n    cudaFree(d_var);\n    cudaFree(d_data);\n    return 0;\n\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T15:30:16.078635Z","iopub.execute_input":"2025-02-21T15:30:16.078977Z","iopub.status.idle":"2025-02-21T15:30:16.084183Z","shell.execute_reply.started":"2025-02-21T15:30:16.078950Z","shell.execute_reply":"2025-02-21T15:30:16.083495Z"}},"outputs":[{"name":"stdout","text":"Overwriting layerNorm.cu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%script bash\nnvcc layerNorm.cu -o red\n./red","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T15:30:16.316823Z","iopub.execute_input":"2025-02-21T15:30:16.317056Z","iopub.status.idle":"2025-02-21T15:30:18.742857Z","shell.execute_reply.started":"2025-02-21T15:30:16.317037Z","shell.execute_reply":"2025-02-21T15:30:18.741975Z"}},"outputs":[{"name":"stdout","text":"gpu Time taken: 0.000318 seconds\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}