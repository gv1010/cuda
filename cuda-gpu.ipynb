{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-17T18:00:48.811597Z","iopub.status.idle":"2025-02-17T18:00:48.811990Z","shell.execute_reply":"2025-02-17T18:00:48.811802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\n!nvcc --version\n!pip install nvcc4jupyter\n%load_ext nvcc4jupyter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:44:43.548356Z","iopub.execute_input":"2025-02-18T17:44:43.548639Z","iopub.status.idle":"2025-02-18T17:46:23.851998Z","shell.execute_reply.started":"2025-02-18T17:44:43.548607Z","shell.execute_reply":"2025-02-18T17:46:23.850999Z"}},"outputs":[{"name":"stdout","text":"Python 3.10.12\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\nCollecting nvcc4jupyter\n  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\nDownloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\nInstalling collected packages: nvcc4jupyter\nSuccessfully installed nvcc4jupyter-1.2.1\nDetected platform \"Kaggle\". Running its setup...\nUpdating the package lists...\nInstalling nvidia-cuda-toolkit, this may take a few minutes...\nSource files will be saved in \"/tmp/tmp352fe0es\".\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:46:23.853191Z","iopub.execute_input":"2025-02-18T17:46:23.853504Z","iopub.status.idle":"2025-02-18T17:46:23.970065Z","shell.execute_reply.started":"2025-02-18T17:46:23.853472Z","shell.execute_reply":"2025-02-18T17:46:23.969326Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!g++ -v","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T07:49:41.391527Z","iopub.execute_input":"2025-02-18T07:49:41.391789Z","iopub.status.idle":"2025-02-18T07:49:41.516411Z","shell.execute_reply.started":"2025-02-18T07:49:41.391756Z","shell.execute_reply":"2025-02-18T07:49:41.515661Z"}},"outputs":[{"name":"stdout","text":"Using built-in specs.\nCOLLECT_GCC=g++\nCOLLECT_LTO_WRAPPER=/usr/lib/gcc/x86_64-linux-gnu/11/lto-wrapper\nOFFLOAD_TARGET_NAMES=nvptx-none:amdgcn-amdhsa\nOFFLOAD_TARGET_DEFAULT=1\nTarget: x86_64-linux-gnu\nConfigured with: ../src/configure -v --with-pkgversion='Ubuntu 11.4.0-1ubuntu1~22.04' --with-bugurl=file:///usr/share/doc/gcc-11/README.Bugs --enable-languages=c,ada,c++,go,brig,d,fortran,objc,obj-c++,m2 --prefix=/usr --with-gcc-major-version-only --program-suffix=-11 --program-prefix=x86_64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --enable-bootstrap --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-vtable-verify --enable-plugin --enable-default-pie --with-system-zlib --enable-libphobos-checking=release --with-target-system-zlib=auto --enable-objc-gc=auto --enable-multiarch --disable-werror --enable-cet --with-arch-32=i686 --with-abi=m64 --with-multilib-list=m32,m64,mx32 --enable-multilib --with-tune=generic --enable-offload-targets=nvptx-none=/build/gcc-11-XeT9lY/gcc-11-11.4.0/debian/tmp-nvptx/usr,amdgcn-amdhsa=/build/gcc-11-XeT9lY/gcc-11-11.4.0/debian/tmp-gcn/usr --without-cuda-driver --enable-checking=release --build=x86_64-linux-gnu --host=x86_64-linux-gnu --target=x86_64-linux-gnu --with-build-config=bootstrap-lto-lean --enable-link-serialization=2\nThread model: posix\nSupported LTO compression algorithms: zlib zstd\ngcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile cpphw.hpp\n\n#include <iostream>\n\nvoid printHelloWorld()\n\n{\n\nstd::cout << \"HelloWorld\\nHelloWorld\";\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:02.460501Z","iopub.execute_input":"2025-02-16T08:52:02.460811Z","iopub.status.idle":"2025-02-16T08:52:02.466794Z","shell.execute_reply.started":"2025-02-16T08:52:02.460774Z","shell.execute_reply":"2025-02-16T08:52:02.466012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile cpphw.cpp\n\n#include \"cpphw.hpp\"\n\nint main()\n\n{\n\nprintHelloWorld();\n\nreturn 0;\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T08:52:02.467816Z","iopub.execute_input":"2025-02-16T08:52:02.468085Z","iopub.status.idle":"2025-02-16T08:52:02.480337Z","shell.execute_reply.started":"2025-02-16T08:52:02.468048Z","shell.execute_reply":"2025-02-16T08:52:02.479536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:08:47.806427Z","iopub.execute_input":"2025-02-11T05:08:47.806741Z","iopub.status.idle":"2025-02-11T05:08:47.925064Z","shell.execute_reply.started":"2025-02-11T05:08:47.806718Z","shell.execute_reply":"2025-02-11T05:08:47.924175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\ng++ cpphw.cpp -std=c++11 -o hw.out\n./hw.out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:08:56.090246Z","iopub.execute_input":"2025-02-11T05:08:56.090696Z","iopub.status.idle":"2025-02-11T05:08:56.389494Z","shell.execute_reply.started":"2025-02-11T05:08:56.090665Z","shell.execute_reply":"2025-02-11T05:08:56.388755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Day 1 (16 Feb, 2025)","metadata":{}},{"cell_type":"code","source":"%%writefile vect_add_cpu.cpp\n// Sequential Vector Addition in CPP\n\n#include <iostream>\n#include <vector>\n#include <ctime>\nusing namespace std;\n\nint main(){\n    clock_t start = clock();\n    int n = 200000000;\n    vector<int> A(n), B(n), C(n);\n    for (int i=0; i<n; i++){\n        A[i] = i+1;\n        B[i] = (i+1)*2;\n    }\n\n    for (int i=0;i<n; i++){\n        C[i] = A[i] + B[i];\n\n    }\n\n    clock_t end = clock();\n\n    double duration = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"Time taken: \" << duration * 1000 << \" milliseconds\" << std::endl; // Convert to milliseconds\n\n    for (int i=0;i <20; i++){\n        cout << C[i] << \" \";\n    }\n    return 0;\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:48:57.183546Z","iopub.execute_input":"2025-02-16T09:48:57.183884Z","iopub.status.idle":"2025-02-16T09:48:57.189127Z","shell.execute_reply.started":"2025-02-16T09:48:57.183856Z","shell.execute_reply":"2025-02-16T09:48:57.188139Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\ng++ vect_add_cpu.cpp -std=c++11 -o cpu.out\n./cpu.out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T09:48:58.107757Z","iopub.execute_input":"2025-02-16T09:48:58.108042Z","iopub.status.idle":"2025-02-16T09:49:04.641248Z","shell.execute_reply.started":"2025-02-16T09:48:58.108021Z","shell.execute_reply":"2025-02-16T09:49:04.640176Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile vect_add_gpu.cu\n// vector addition on gpu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void vector_addition(int *A, int *B, int *C, int n){\n    // std::cout << \"blockIdx.x = \" << blockIdx.x << endl ;\n    // std::cout << \"blockDim.x = \" << blockDim.x << endl ;\n    // std::cout << \"threadIdx.x = \" << threadIdx.x << endl ;\n\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i<n){\n        C[i] = A[i] + B[i];\n    }\n    \n};\n\nint main(){\n    int n = 200000000;\n    vector<int> A(n), B(n), C(n);\n    for (int i=0; i<n; i++){\n        A[i] = i+1;\n        B[i] = (i+1)*2;\n    }\n\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    cudaMalloc((void **)&d_A, n*sizeof(int));\n    cudaMalloc((void **)&d_B, n*sizeof(int));\n    cudaMalloc((void **)&d_C, n*sizeof(int));\n\n    cudaMemcpy(d_A, A.data(), n*sizeof(int), cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, B.data(), n*sizeof(int), cudaMemcpyHostToDevice);\n\n    int blockSize = 2;\n    int gridSize = n+blockSize - 1;\n    \n    vector_addition <<<gridSize, blockSize>>>(d_A, d_B, d_C, n);\n    cudaMemcpy(C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost);\n\n    clock_t end = clock();\n\n    double duration = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"Time taken: \" << duration * 1000 << \" milliseconds\" << std::endl; // Convert to milliseconds\n\n    for (int i=0;i <20; i++){\n        cout << C[i] << \" \";\n    }\n    cudaFree(d_A);\n    cudaFree(d_B);\n    cudaFree(d_C);\n    \n    return 0;  \n}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:07:07.363220Z","iopub.execute_input":"2025-02-16T11:07:07.363529Z","iopub.status.idle":"2025-02-16T11:07:07.369766Z","shell.execute_reply.started":"2025-02-16T11:07:07.363507Z","shell.execute_reply":"2025-02-16T11:07:07.368812Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%script bash\nnvcc vect_add_gpu.cu -o kernel\n./kernel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T11:07:11.333063Z","iopub.execute_input":"2025-02-16T11:07:11.333322Z","iopub.status.idle":"2025-02-16T11:07:19.223138Z","shell.execute_reply.started":"2025-02-16T11:07:11.333301Z","shell.execute_reply":"2025-02-16T11:07:19.222288Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Notes:\n* Gird Dimension: gridDim.x, gridDim.y, gridDim.z is the number of blocks in each direction of Grid.\n* Block Dimension: blockDim.x, blockDim.y, blockDim.z, is the number of threads in each direction of block.\n* Block Index: blockIdx.x[y,z], index of the block within the grid\n* Thread Index: threadIdx.x[y,z], index of thread within a block\n  \nthreadId =\n\n            blockDim.x\\*blockIdx.x + blockDim.x\\*threadIdx.y + threadIdx.x (simple 2D within a block) +\n              \n            gridDim.x\\*blockDim.y\\*blockIdx.y (blocks of threads in previous row) \n\n\n**Steps:**\n1. Define the function to run on GPU.\n2. Declare variables on CPU and allocate necessary memory on GPU\n3. Copy the data from CPU to GPU on the allocated locations.\n4. Call the kernel\n5. Save the copy of the task result on the CPU.\n6. free the memory on GPU\n","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"markdown","source":"# Day 2 (17 Feb, 2025)","metadata":{"execution":{"iopub.status.busy":"2025-02-17T17:26:03.433641Z","iopub.execute_input":"2025-02-17T17:26:03.434097Z","iopub.status.idle":"2025-02-17T17:26:03.439567Z","shell.execute_reply.started":"2025-02-17T17:26:03.434055Z","shell.execute_reply":"2025-02-17T17:26:03.437465Z"},"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"%%writefile vect_2D_add_gpu.cu\n// vector addition on gpu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void vector_addition(int *A, int *B, int *C, int rows, int cols){\n    // std::cout << \"blockIdx.x = \" << blockIdx.x << endl ;\n    // std::cout << \"blockDim.x = \" << blockDim.x << endl ;\n    // std::cout << \"threadIdx.x = \" << threadIdx.x << endl ;\n\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (row < rows && col < cols){\n        int index = row*cols + col;\n       // std::cout << index << endl;\n        C[index] = A[index] + B[index];\n    }\n    \n};\n\nvoid print_vector(vector<vector <int>> matrix){\n  // vector<vector<int>> matrix(3);\n    for (vector vec: matrix){\n      for (int val : vec){\n        cout << val << \" \";\n      }\n      cout << endl;\n    }\n}\n\n// Helper function to check for CUDA errors\nvoid checkCudaError(cudaError_t error) {\n    if (error != cudaSuccess) {\n        std::cerr << \"CUDA Error: \" << cudaGetErrorString(error) << std::endl;\n        exit(EXIT_FAILURE);\n    }\n}\n\nint main(){\n    \n    int rows = 30000;\n    int cols = 30000;\n    int n = rows*cols;\n    vector<int> h_A(n), h_B(n), h_C(n);\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_A[i * cols + j] = 2; // Example initialization for A\n            h_B[i * cols + j] = -1;\n                }\n    }\n\n    clock_t start_cpu = clock();\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_C[i * cols + j] = h_A[i * cols + j] + h_B[i * cols + j];\n                }\n    }\n    clock_t end_cpu = clock();\n    double duration_cpu = static_cast<double>(end_cpu - start_cpu) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"CPU Time taken: \" << duration_cpu * 1000 << \" milliseconds\" << std::endl; \n\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    checkCudaError(cudaMalloc((void **)&d_A, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_B, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_C, n*sizeof(int)));\n\n    checkCudaError(cudaMemcpy(d_A, h_A.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n    checkCudaError(cudaMemcpy(d_B, h_B.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n\n    // dim3 blockSize(rows, cols); // Block dimension (threads per block in x and y) - adjust based on matrix size and GPU capabilities\n    // dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n\n    dim3 blockSize(16, 16); // Example block size\n    blockSize.x = min(blockSize.x, cols); // Clamp block size to cols\n    blockSize.y = min(blockSize.y, rows); // Clamp block size to rows\n    dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n    \n    vector_addition <<<gridSize, blockSize>>>(d_A, d_B, d_C, rows, cols);\n    checkCudaError(cudaGetLastError());\n    clock_t d2h_start = clock();\n    checkCudaError(cudaMemcpy(h_C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost));\n    clock_t d2h_end = clock();\n    double d2h_duration = static_cast<double>(d2h_end - d2h_start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"MemCpy D2H Time taken: \" << d2h_duration * 1000 << \" milliseconds\" << std::endl; \n\n    clock_t end = clock();\n\n    double duration_cuda = static_cast<double>(end - start) / CLOCKS_PER_SEC; // Time in seconds\n\n    std::cout << \"CUDA Time taken: \" << duration_cuda * 1000 << \" milliseconds\" << std::endl; \n    // Convert to milliseconds\n\n    cout << \"CUDA Matrix Addition (2D):\" << endl;\n    \n    for (int i = 0; i < 10; ++i) {\n        for (int j = 0; j < 10; ++j) {\n            cout << h_C[i * cols + j] << \" \";\n        }\n        cout << endl;\n    }\n    checkCudaError(cudaFree(d_A));\n    checkCudaError(cudaFree(d_B));\n    checkCudaError(cudaFree(d_C));\n    checkCudaError(cudaDeviceSynchronize());\n    \n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:37.930699Z","iopub.execute_input":"2025-02-18T08:38:37.931034Z","iopub.status.idle":"2025-02-18T08:38:37.936322Z","shell.execute_reply.started":"2025-02-18T08:38:37.931004Z","shell.execute_reply":"2025-02-18T08:38:37.935625Z"}},"outputs":[{"name":"stdout","text":"Overwriting vect_2D_add_gpu.cu\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"%%script bash\nnvcc vect_2D_add_gpu.cu -o vect_2d\n./vect_2d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T08:38:41.025379Z","iopub.execute_input":"2025-02-18T08:38:41.025651Z","iopub.status.idle":"2025-02-18T08:39:14.104315Z","shell.execute_reply.started":"2025-02-18T08:38:41.025629Z","shell.execute_reply":"2025-02-18T08:39:14.103417Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"CPU Time taken: 7124.99 milliseconds\nMemCpy D2H Time taken: 818.66 milliseconds\nCUDA Time taken: 2489.67 milliseconds\nCUDA Matrix Addition (2D):\n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n1 1 1 1 1 1 1 1 1 1 \n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Day 3 (18 Feb, 2025)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Chapter 3 Notes:\n* dim3 dimGrid(32,1,1), dim dimBlock(128,1,1) or dim3 dog(32,1,1), dim3 cat(128,1,1)\n* vec_add <<<dimGrid or dog, dimBlock or cat>>> // we can use both with dim3 type, it is vector of int type with 3 elements, x, y, z.\n* \"For convenience, CUDA provides a special shortcut for calling a kernel with one-dimensional (1D) grids and blocks. Instead of using dim3 variables, one can use arithmetic expressions to specify the configuration of 1D grids and blocks. In this case, the CUDA compiler simply takes the arithmetic expression as the x dimensions and assumes that the y and z dimensions are 1.\" Page 49\n* \"the gridDim and blockDim are built-in variables in a kernel and always reflect the dimensions of the grid and the blocks, respectively.\"\n* the allowed values of gridDim.x range from 1 to 231 2 1,1 and those of gridDim.y and gridDim.z range from 1 to 216 2 1 (65,535).\n* The total size of a block in current CUDA systems is limited to 1024 threads. These threads can be distributed across the three dimensions in any way as long as the total number of threads does not exceed 1024.\n* We need to know the no of cols at the compile time to accept dynamically allocated  arrays, but we do not have this info, as a result we flattne the dynamically allocated 2D arrays into an equivalent 1D arrays.\n* The linearized access to a 3D array P will be in the form of P[plane * m * n +row * m + col].","metadata":{}},{"cell_type":"code","source":"%%writefile matrix_mul.cu\n\n#include <iostream>\n#include <vector>\n#include <ctime>\n#include <cuda_runtime.h>\n\nusing namespace std;\n\n__global__ void mat_mul(int *P_A, int *P_B,int *P_C, int Width){\n    int row = blockDim.y*blockIdx.y + threadIdx.y;\n    int col = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (row < Width && col < Width){\n        for (int k=0; k<Width; k++){\n            P_C[row*Width+col] += P_A[row*Width+k] * P_B[k*Width+col];\n        }\n    }\n}\n\nvoid checkCudaError(cudaError_t error) {\n    if (error != cudaSuccess) {\n        std::cerr << \"CUDA Error: \" << cudaGetErrorString(error) << std::endl;\n        exit(EXIT_FAILURE);\n    }\n}\nint main(){\n    int rows = 3000;\n    int cols = 3000;\n    int n = rows*cols;\n    vector<int> h_A(n), h_B(n), h_C(n);\n    for (int i = 0; i < rows; ++i) {\n        for (int j = 0; j < cols; ++j) {\n            h_A[i * cols + j] = 1; // Example initialization for A\n            h_B[i * cols + j] = 2;\n                }\n    }\n    clock_t start = clock();\n    int *d_A, *d_B, *d_C;\n    checkCudaError(cudaMalloc((void **)&d_A, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_B, n*sizeof(int)));\n    checkCudaError(cudaMalloc((void **)&d_C, n*sizeof(int)));\n\n    checkCudaError(cudaMemcpy(d_A, h_A.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n    checkCudaError(cudaMemcpy(d_B, h_B.data(), n*sizeof(int), cudaMemcpyHostToDevice));\n\n    int width = 16;\n    dim3 blockSize(32, 32); // Example block size\n    blockSize.x = min(blockSize.x, cols); // Clamp block size to cols\n    blockSize.y = min(blockSize.y, rows); // Clamp block size to rows\n    dim3 gridSize((cols + blockSize.x - 1) / blockSize.x, (rows + blockSize.y - 1) / blockSize.y);\n    \n    mat_mul <<<gridSize, blockSize>>>(d_A, d_B, d_C, width);\n    \n    clock_t d2h_start = clock();\n    checkCudaError(cudaMemcpy(h_C.data(), d_C, n*sizeof(int), cudaMemcpyDeviceToHost));\n    clock_t d2h_end = clock();\n    double d2h_duration = static_cast<double>(d2h_end - d2h_start) / CLOCKS_PER_SEC; // Time in seconds\n    std::cout << \"MemCpy D2H Time taken: \" << d2h_duration * 1000 << \" milliseconds\" << std::endl; \n\n    checkCudaError(cudaGetLastError());\n    cout << \"CUDA Matrix Mul (2D):\" << endl;\n    \n    for (int i = 0; i < 10; ++i) {\n        for (int j = 0; j < 10; ++j) {\n            cout << h_C[i * cols + j] << \" \";\n        }\n        cout << endl;\n    }\n    return 0;\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:57:44.603922Z","iopub.execute_input":"2025-02-18T17:57:44.604226Z","iopub.status.idle":"2025-02-18T17:57:44.609622Z","shell.execute_reply.started":"2025-02-18T17:57:44.604197Z","shell.execute_reply":"2025-02-18T17:57:44.608714Z"}},"outputs":[{"name":"stdout","text":"Overwriting matrix_mul.cu\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"%%script bash\nnvcc matrix_mul.cu -o mul\n./mul","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:57:46.880313Z","iopub.execute_input":"2025-02-18T17:57:46.880621Z","iopub.status.idle":"2025-02-18T17:57:49.452680Z","shell.execute_reply.started":"2025-02-18T17:57:46.880596Z","shell.execute_reply":"2025-02-18T17:57:49.451574Z"}},"outputs":[{"name":"stdout","text":"MemCpy D2H Time taken: 8.626 milliseconds\nCUDA Matrix Mul (2D):\n32 32 32 32 32 32 32 32 32 32 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n0 0 0 0 0 0 0 0 0 0 \n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!nvcc --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T17:47:46.450185Z","iopub.execute_input":"2025-02-18T17:47:46.450503Z","iopub.status.idle":"2025-02-18T17:47:46.568713Z","shell.execute_reply.started":"2025-02-18T17:47:46.450477Z","shell.execute_reply":"2025-02-18T17:47:46.567735Z"}},"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Aug_15_22:02:13_PDT_2023\nCuda compilation tools, release 12.2, V12.2.140\nBuild cuda_12.2.r12.2/compiler.33191640_0\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}